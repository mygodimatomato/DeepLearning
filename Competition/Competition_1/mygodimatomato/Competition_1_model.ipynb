{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import ast\n",
    "import itertools\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "n = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Month', 'Weekday', 'Year', 'Day', 'Hour', 'title_bit_count', 'img_count', 'link_count', 'title_word_count', 'content_len', 'content_word_count'\n",
    "\n",
    "-----------------------------------------------------------\n",
    "括號內是 acc > 0.58 的權重\n",
    "'Month', 'Weekday', 'Year', 'Hour' -> 100% (100%)\n",
    "\n",
    "'Day' -> 85% (72%)\n",
    "\n",
    "'img_count' -> 89% (65%)\n",
    "\n",
    "'channel' -> 81%\n",
    "\n",
    "'content_len' -> 49% (48%) -> 59%\n",
    "\n",
    "'author' -> 56%\n",
    "\n",
    "'title_bit_count' -> 36% (48%) -> 53%\n",
    "\n",
    "'link_count' -> 40% (46%) -> 37%\n",
    "\n",
    "'title_word_count' -> 36% (49%) -> 37% (drop?)\n",
    "\n",
    "'content_word_count' -> 38% (47%) (drop)\n",
    "\n",
    "'categories_count' -> 16% (15%) (drop)\n",
    "\n",
    "'Minutes' -> 跟 categories_count 差不多 (drop)\n",
    "\n",
    "**author and channel is important**\n",
    "channel 權重大於 author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets_processed/train_processed_2.csv')\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra features : title, content, categories\n",
    "X_train = df[['Month', 'Weekday', 'Year', 'Day', 'Hour', 'img_count', 'content_len', 'channel', 'author', 'categories']]\n",
    "y_train = df.iloc[:]['Popularity'].values\n",
    "y_train[y_train==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['title'] = X_train['title'].apply(ast.literal_eval)\n",
    "X_train['categories'] = X_train['categories'].apply(ast.literal_eval)\n",
    "X_train['channel'] = X_train['channel'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenizer_author(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    authors = re.split(',', text)\n",
    "    for idx, author in enumerate(authors):\n",
    "        authors[idx] = re.sub(' ', '', author)\n",
    "    return authors\n",
    "    \n",
    "def tokenizer_channel(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    channels = re.split(',', text)\n",
    "    for idx, channel in enumerate(channels):\n",
    "        channels[idx] = re.sub(' ', '', channel)\n",
    "    return channels\n",
    "\n",
    "def tokenizer_title(list):\n",
    "    # word-stemming\n",
    "    stemmed_title = [stemmer.stem(word) for word in list]\n",
    "    # remove stopwords\n",
    "    cleaned_title = [word for word in stemmed_title if word not in stop]\n",
    "    return cleaned_title\n",
    "\n",
    "\n",
    "def tokenizer_categories(list):   \n",
    "    # word-stemming\n",
    "    stemmed_categories = [stemmer.stem(word) for word in list]\n",
    "    # remove stopwords\n",
    "    cleaned_categories = [word for word in stemmed_categories if word not in stop]\n",
    "    return cleaned_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train['title'] = X_train['title'].apply(tokenizer_title)\n",
    "# X_train['title'].to_csv('../datasets_processed/title.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the author\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer_author, lowercase=False)\n",
    "vectorized_data = vectorizer.fit_transform(X_train['author'])\n",
    "vectorized_author = pd.DataFrame(vectorized_data.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# preprocess the channel\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer_channel, lowercase=False)\n",
    "vectorized_data = vectorizer.fit_transform(X_train['channel'])\n",
    "vectorized_channel = pd.DataFrame(vectorized_data.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# preprocess the title\n",
    "# vectorizer = CountVectorizer(tokenizer=tokenizer_title, lowercase=False)\n",
    "# vectorized_data = vectorizer.fit_transform(X_train['title'])\n",
    "# vectorized_title = pd.DataFrame(vectorized_data.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# preprocess the categories\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer_categories, lowercase=False)\n",
    "vectorized_data = vectorizer.fit_transform(X_train['categories'])\n",
    "vectorized_categories = pd.DataFrame(vectorized_data.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vectorized_categories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing standardization for numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train[['Month', 'Weekday', 'Year', 'Day', 'Hour', 'img_count', 'content_len', ]] = scaler.fit_transform(X_train[['Month', 'Weekday', 'Year', 'Day', 'Hour', 'img_count', 'content_len']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['author'], axis=1)\n",
    "X_train = X_train.drop(['channel'], axis=1)\n",
    "X_train = X_train.drop(['categories'], axis=1)\n",
    "# X_train = pd.concat([X_train, vectorized_author], axis=1)\n",
    "# X_train = pd.concat([X_train, vectorized_channel], axis=1)\n",
    "X_train = pd.concat([X_train, vectorized_categories], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbgm = LGBMClassifier(n_estimators=n, max_depth=10, learning_rate=0.1, random_state=0)\n",
    "scores = cross_val_score(estimator=lbgm, X=X_train.values, y=y_train, cv=5, scoring='roc_auc')\n",
    "print('%.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = []\n",
    "acc = []\n",
    "attributes = X_train.columns.values\n",
    "for r in range(1, len(attributes) + 1):\n",
    "    for combination in itertools.combinations(attributes, r):\n",
    "        # if combination.__contains__('Month') and combination.__contains__('Weekday') and combination.__contains__('Year') and \\\n",
    "        #     combination.__contains__('Day') and combination.__contains__('Hour') and combination.__contains__('img_count') and \\\n",
    "        #     combination.__contains__('channel'):\n",
    "            tmp = X_train[list(combination)]\n",
    "            if combination.__contains__('author'):\n",
    "                tmp = tmp.drop(['author'], axis=1)\n",
    "                tmp = pd.concat([tmp, vectorized_author], axis=1)\n",
    "            if combination.__contains__('categories'):\n",
    "            # if combination.__contains__('title'):\n",
    "            #     tmp = tmp.drop(['title'], axis=1)\n",
    "            #     tmp = pd.concat([tmp, vectorized_title], axis=1)\n",
    "            if combination.__contains__('channel'):\n",
    "                tmp = tmp.drop(['channel'], axis=1)\n",
    "                tmp = pd.concat([tmp, vectorized_channel], axis=1)\n",
    "            # if combination.__contains__('content'):\n",
    "            lbgm = LGBMClassifier(n_estimators=n, max_depth=10, learning_rate=0.1, random_state=1, num_leaves=100)\n",
    "            scores = cross_val_score(estimator=lbgm, X=tmp.values, y=y_train, cv=5, scoring='roc_auc')\n",
    "            com.append(combination)\n",
    "            acc.append([scores.mean(), scores.std()])\n",
    "\n",
    "\n",
    "tmp = pd.DataFrame({'combination': com, 'accuracy': acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output validation result\n",
    "sorted_tmp = tmp.sort_values(by=['accuracy'], ascending=False)\n",
    "sorted_tmp.to_csv('../training_output/lgbm_2_acc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10, random_state=15)\n",
    "scores = cross_val_score(estimator=rf, X=tmp.values, y=y_train, cv=5, scoring='roc_auc')\n",
    "print('%.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = []\n",
    "acc = []\n",
    "attributes = X_train.columns.values\n",
    "for r in range(1, len(attributes) + 1):\n",
    "    for combination in itertools.combinations(attributes, r):\n",
    "        # if combination.__contains__('Month') and combination.__contains__('Weekday') and combination.__contains__('Year') and \\\n",
    "        #     combination.__contains__('Day') and combination.__contains__('Hour') and combination.__contains__('img_count') and \\\n",
    "        #     combination.__contains__('channel'):\n",
    "            tmp = X_train[list(combination)]\n",
    "            if combination.__contains__('author'):\n",
    "                tmp = tmp.drop(['author'], axis=1)\n",
    "                tmp = pd.concat([tmp, vectorized_author], axis=1)\n",
    "            # if combination.__contains__('categories'):\n",
    "            # if combination.__contains__('title'):\n",
    "            #     tmp = tmp.drop(['title'], axis=1)\n",
    "            #     tmp = pd.concat([tmp, vectorized_title], axis=1)\n",
    "            if combination.__contains__('channel'):\n",
    "                tmp = tmp.drop(['channel'], axis=1)\n",
    "                tmp = pd.concat([tmp, vectorized_channel], axis=1)\n",
    "            rf = RandomForestClassifier(n_estimators=10, random_state=15)\n",
    "            scores = cross_val_score(estimator=rf, X=tmp.values, y=y_train, cv=5, scoring='roc_auc')\n",
    "            com.append(combination)\n",
    "            acc.append([scores.mean(), scores.std()])\n",
    "\n",
    "\n",
    "tmp = pd.DataFrame({'combination': com, 'accuracy': acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({'combination': com, 'accuracy': acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output validation result\n",
    "sorted_tmp = tmp.sort_values(by=['accuracy'], ascending=False)\n",
    "sorted_tmp.to_csv('../training_output/random_forest_acc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(estimator=knn, X=X_train.values, y=y_train, cv=5, scoring='roc_auc')\n",
    "print('%.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# X_train = pd.concat([X_train, vectorized_author], axis=1)\n",
    "# X_train = pd.concat([X_train, vectorized_channel], axis=1)\n",
    "# X_train = pd.concat([X_train, vectorized_categories], axis=1)\n",
    "X_train = df[['Month', 'Weekday', 'Year', 'Day', 'Hour', 'img_count', 'categories', 'author', 'channel']]\n",
    "com = []\n",
    "acc = []\n",
    "attributes = X_train.columns.values\n",
    "for r in range(1, len(attributes) + 1):\n",
    "    for combination in itertools.combinations(attributes, r):\n",
    "      if combination.__contains__('Month') and combination.__contains__('Weekday') and combination.__contains__('Year') :\n",
    "        tmp = X_train[list(combination)]\n",
    "        if combination.__contains__('author'):\n",
    "          tmp = tmp.drop(['author'], axis=1)\n",
    "          tmp = pd.concat([tmp, vectorized_author], axis=1)\n",
    "        if combination.__contains__('categories'):\n",
    "          tmp = tmp.drop(['categories'], axis=1)\n",
    "          tmp = pd.concat([tmp, vectorized_categories], axis=1)\n",
    "        if combination.__contains__('channel'):\n",
    "          tmp = tmp.drop(['channel'], axis=1)\n",
    "          tmp = pd.concat([tmp, vectorized_channel], axis=1)\n",
    "        knn = KNeighborsClassifier(n_neighbors=400)\n",
    "        scores = cross_val_score(estimator=knn, X=tmp.values, y=y_train, cv=5, scoring='roc_auc')\n",
    "        com.append(combination)\n",
    "        acc.append([scores.mean(), scores.std()])\n",
    "        print('%.3f (+/-%.3f)' % (scores.mean(), scores.std()))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output validation result\n",
    "tmp = pd.DataFrame({'combination': com, 'accuracy': acc})\n",
    "sorted_tmp = tmp.sort_values(by=['accuracy'], ascending=False)\n",
    "sorted_tmp.to_csv('../training_output/KNN_600_acc.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
