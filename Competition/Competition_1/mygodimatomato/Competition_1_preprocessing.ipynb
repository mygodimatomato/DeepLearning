{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/train.csv')\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_preprocess(text):\n",
    "    text = re.sub('By', '', text)\n",
    "    text = re.sub('by', '', text)\n",
    "    text = re.sub(',', ' ,', text)\n",
    "    text = re.sub(' and ', ' , ', text)\n",
    "    text = re.sub('&', ',', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content  \\\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...   \n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...   \n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...   \n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...   \n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...   \n",
      "\n",
      "   img_count  link_count                                              title  \\\n",
      "0          1          22  NASA's Grand Challenge: Stop Asteroids From De...   \n",
      "1          2          18  Google's New Open Source Patent Pledge: We Won...   \n",
      "2          2          11  Ballin': 2014 NFL Draft Picks Get to Choose Th...   \n",
      "3          1          13        Cameraperson Fails Deliver Slapstick Laughs   \n",
      "4         52          16  NFL Star Helps Young Fan Prove Friendship With...   \n",
      "\n",
      "   title_word_count  title_bit_count  \\\n",
      "0                 8               53   \n",
      "1                12               63   \n",
      "2                12               57   \n",
      "3                 5               39   \n",
      "4                10               52   \n",
      "\n",
      "                                             content  content_len  ...  \\\n",
      "0   there may be killer asteroids headed for eart...         3591  ...   \n",
      "1   google took a stand of sorts against patent-l...         1843  ...   \n",
      "2   you've spend countless hours training to be a...         6646  ...   \n",
      "3       tired of the same old sports fails and ne...         1821  ...   \n",
      "4   at 6-foot-5 and 298 pounds , all-pro nfl star...         8919  ...   \n",
      "\n",
      "         channel             author  Weekday  Year Month  Day  Hour  Minutes  \\\n",
      "0          world    Clara Moskowitz        2  2013     6   19    15        4   \n",
      "1           tech   Christina Warren        3  2013     3   28    17       40   \n",
      "2  entertainment          Sam Laird        2  2014     5    7    19       15   \n",
      "3    watercooler          Sam Laird        4  2013    10   11     2       26   \n",
      "4  entertainment    Connor Finnegan        3  2014     4   17     3       31   \n",
      "\n",
      "   Sec  Timezone  \n",
      "0   30       UTC  \n",
      "1   55       UTC  \n",
      "2   20       UTC  \n",
      "3   50       UTC  \n",
      "4   43       UTC  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def attribute_attract(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # extract images\n",
    "    images = soup.find_all('img')\n",
    "    # extract links\n",
    "    links = soup.find_all('a')\n",
    "    \n",
    "    # extract title\n",
    "    h1_tag = soup.find('h1', {'class': 'title'})\n",
    "    title = \"\"\n",
    "    if h1_tag is not None:\n",
    "        title = h1_tag.text\n",
    "    title_bit = len(title)\n",
    "    words = title.split()\n",
    "    title_word_count = len(words)\n",
    "    title_bit_count = title_bit - title_word_count + 1\n",
    "\n",
    "    # extract content\n",
    "    content = soup.find('section', {'class':'article-content'}).text\n",
    "    content_len = len(content)\n",
    "    content_words = len(content.split())\n",
    "    content = re.sub('topics: ', '', content.lower())\n",
    "    content = re.sub(',', ' ,', content)\n",
    "\n",
    "    # extract categories \n",
    "    categories = []\n",
    "\n",
    "        # Find the <a> tags\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        # Extract the href attribute\n",
    "        href_value = a_tag['href']\n",
    "        \n",
    "        # Use regex to extract the category\n",
    "        match = re.search(r'category/(.*)/', href_value)\n",
    "        \n",
    "        if match:\n",
    "            categories.append(match.group(1))\n",
    "    \n",
    "    # extract channel\n",
    "    channel_tag = soup.find('meta', {'property': 'article:section'})\n",
    "    article_tag = soup.find('article', {'data-channel': True})\n",
    "    channel = article_tag['data-channel']\n",
    "\n",
    "    article_info = soup.head.find('div', {'class': 'article-info'})\n",
    "    author = article_info.find('span', {'class': 'author_name'})\n",
    "    if author != None:\n",
    "        author = author.get_text()\n",
    "    elif article_info.span != None:\n",
    "        author = article_info.span.string\n",
    "    else:\n",
    "        author = article_info.a.string\n",
    "    author = author_preprocess(author)\n",
    "\n",
    "    # time data\n",
    "    time_tag = soup.find('time', datetime=True)\n",
    "    if time_tag is None:\n",
    "        return -1\n",
    "    datetime_str = time_tag['datetime']\n",
    "    datetime_obj = datetime.strptime(datetime_str, \"%a, %d %b %Y %H:%M:%S %z\")\n",
    "\n",
    "    day_of_week = datetime_obj.weekday()\n",
    "    year = datetime_obj.year\n",
    "    month = datetime_obj.month\n",
    "    day = datetime_obj.day\n",
    "    hour = datetime_obj.hour\n",
    "    minutes = datetime_obj.minute\n",
    "    sec = datetime_obj.second\n",
    "    timezone = datetime_obj.tzinfo.tzname(datetime_obj)\n",
    "\n",
    "    return len(images), len(links), title, title_word_count, title_bit_count, content, content_len, content_words, categories, len(categories), channel, author, day_of_week, year, month, day, hour, minutes, sec, timezone\n",
    "\n",
    "df[['img_count', 'link_count', 'title', 'title_word_count', 'title_bit_count', 'content', 'content_len', 'content_word_count', 'categories', 'categories_count', 'channel', 'author', 'Weekday', 'Year', 'Month', 'Day', 'Hour', 'Minutes', 'Sec', 'Timezone']] = df['Page content'].apply(attribute_attract).apply(pd.Series)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe need it, not sure\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "\n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mygodimatomato/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "df['title'] = df['title'].apply(tokenizer_stem_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuations(lst):\n",
    "    punctuations = set(string.punctuation)\n",
    "    new_list = []\n",
    "\n",
    "    for item in lst:\n",
    "        new_item = ''.join([char for char in item if char not in punctuations])\n",
    "        new_list.append(new_item)\n",
    "\n",
    "    return new_list\n",
    "\n",
    "df['title'] = df['title'].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content  \\\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...   \n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...   \n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...   \n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...   \n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...   \n",
      "\n",
      "   img_count  link_count                                              title  \\\n",
      "0          1          22  [nasa, grand, challenge, stop, asteroid, from,...   \n",
      "1          2          18  [google, new, open, sourc, patent, pledge, we,...   \n",
      "2          2          11  [ballin, nfl, draft, pick, get, choos, their, ...   \n",
      "3          1          13      [cameraperson, fail, deliv, slapstick, laugh]   \n",
      "4         52          16  [nfl, star, help, young, fan, prove, friendshi...   \n",
      "\n",
      "   title_word_count  title_bit_count  \\\n",
      "0                 8               53   \n",
      "1                12               63   \n",
      "2                12               57   \n",
      "3                 5               39   \n",
      "4                10               52   \n",
      "\n",
      "                                             content  content_len  ...  \\\n",
      "0   there may be killer asteroids headed for eart...         3591  ...   \n",
      "1   google took a stand of sorts against patent-l...         1843  ...   \n",
      "2   you've spend countless hours training to be a...         6646  ...   \n",
      "3       tired of the same old sports fails and ne...         1821  ...   \n",
      "4   at 6-foot-5 and 298 pounds , all-pro nfl star...         8919  ...   \n",
      "\n",
      "         channel             author  Weekday  Year Month  Day  Hour  Minutes  \\\n",
      "0          world    Clara Moskowitz        2  2013     6   19    15        4   \n",
      "1           tech   Christina Warren        3  2013     3   28    17       40   \n",
      "2  entertainment          Sam Laird        2  2014     5    7    19       15   \n",
      "3    watercooler          Sam Laird        4  2013    10   11     2       26   \n",
      "4  entertainment    Connor Finnegan        3  2014     4   17     3       31   \n",
      "\n",
      "   Sec  Timezone  \n",
      "0   30       UTC  \n",
      "1   55       UTC  \n",
      "2   20       UTC  \n",
      "3   50       UTC  \n",
      "4   43       UTC  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../datasets_processed/train_processed_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Id                                       Page content  img_count  \\\n",
      "0  27643  <html><head><div class=\"article-info\"><span cl...        1.0   \n",
      "1  27644  <html><head><div class=\"article-info\"><span cl...        3.0   \n",
      "2  27645  <html><head><div class=\"article-info\"><span cl...        2.0   \n",
      "3  27646  <html><head><div class=\"article-info\"><span cl...        1.0   \n",
      "4  27647  <html><head><div class=\"article-info\"><span cl...        1.0   \n",
      "\n",
      "   link_count                                              title  \\\n",
      "0        30.0  Soccer Star Gets Twitter Death Threats After T...   \n",
      "1        13.0               Google Glass Gets an Accessory Store   \n",
      "2        13.0     OUYA Gaming Console Already Sold Out on Amazon   \n",
      "3        15.0           'Between Two Ferns' Mocks Oscar Nominees   \n",
      "4        10.0  'American Sniper' Trailer: Looks Like Eastwood...   \n",
      "\n",
      "   title_word_count  title_bit_count  \\\n",
      "0              11.0             64.0   \n",
      "1               6.0             31.0   \n",
      "2               8.0             39.0   \n",
      "3               6.0             35.0   \n",
      "4              12.0             59.0   \n",
      "\n",
      "                                             content  content_len  \\\n",
      "0       note to humanity: one direction fandom ai...       3372.0   \n",
      "1   shortly after announcing a hardware upgrade f...        821.0   \n",
      "2   well , that was quick. just hours after going...        990.0   \n",
      "3    between two ferns: oscar buzz edition part 1...       1015.0   \n",
      "4       ever since the hurt locker it seems like ...       1328.0   \n",
      "\n",
      "   content_word_count  ...        channel           author Weekday    Year  \\\n",
      "0               527.0  ...  entertainment        Sam Laird     0.0  2013.0   \n",
      "1               142.0  ...           tech   Stan Schroeder     3.0  2013.0   \n",
      "2               164.0  ...       business   Todd Wasserman     1.0  2013.0   \n",
      "3               153.0  ...           film     Neha Prakash     2.0  2013.0   \n",
      "4               219.0  ...  entertainment      Josh Dickey     4.0  2014.0   \n",
      "\n",
      "   Month   Day  Hour  Minutes   Sec  Timezone  \n",
      "0    9.0   9.0  19.0     47.0   2.0       UTC  \n",
      "1   10.0  31.0   9.0     25.0   2.0       UTC  \n",
      "2    6.0  25.0  12.0     54.0  54.0       UTC  \n",
      "3    2.0  13.0   3.0     30.0  21.0       UTC  \n",
      "4   10.0   3.0   1.0     34.0  54.0       UTC  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/test.csv')\n",
    "df[['img_count', 'link_count', 'title', 'title_word_count', 'title_bit_count', 'content', 'content_len', 'content_word_count', 'categories', 'categories_count', 'channel', 'author', 'Weekday', 'Year', 'Month', 'Day', 'Hour', 'Minutes', 'Sec', 'Timezone']] = df['Page content'].apply(attribute_attract).apply(pd.Series)\n",
    "# df['title'] = df['title'].apply(tokenizer_stem_nostop)\n",
    "# df['title'] = df['title'].apply(remove_punctuations)\n",
    "df.to_csv('../datasets_processed/test_processed_2.csv', index=False)\n",
    "\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygodimatomato",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
