{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/train.csv')\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract df[0] for checking the content\n",
    "tmp = df['Page content'][1]\n",
    "tmp = BeautifulSoup(tmp, 'html.parser')\n",
    "with open(\"content2.html\", \"w\") as f:\n",
    "    f.write(str(tmp.prettify()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content  \\\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...   \n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...   \n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...   \n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...   \n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...   \n",
      "\n",
      "   img_count  link_count                                              title  \\\n",
      "0          1          22  NASA's Grand Challenge: Stop Asteroids From De...   \n",
      "1          2          18  Google's New Open Source Patent Pledge: We Won...   \n",
      "2          2          11  Ballin': 2014 NFL Draft Picks Get to Choose Th...   \n",
      "3          1          13        Cameraperson Fails Deliver Slapstick Laughs   \n",
      "4         52          16  NFL Star Helps Young Fan Prove Friendship With...   \n",
      "\n",
      "                                          categories  categories_count  \\\n",
      "0  [asteroid, asteroids, challenge, earth, space,...                 7   \n",
      "1  [patent-lawsuit-theater, apps-software, google...                 9   \n",
      "2  [nfl, espn, entertainment, nfl, nfl-draft, spo...                 7   \n",
      "3  [youtube, sports, video, videos-watercooler, w...                 5   \n",
      "4  [nfl, instagram, entertainment, instagram, ins...                 7   \n",
      "\n",
      "         channel  \n",
      "0          world  \n",
      "1           tech  \n",
      "2  entertainment  \n",
      "3    watercooler  \n",
      "4  entertainment  \n"
     ]
    }
   ],
   "source": [
    "def attribute_attract(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # extract images\n",
    "    images = soup.find_all('img')\n",
    "    # extract links\n",
    "    links = soup.find_all('a')\n",
    "    \n",
    "    # extract title\n",
    "    h1_tag = soup.find('h1', {'class': 'title'})\n",
    "    title = \"\"\n",
    "    if h1_tag is not None:\n",
    "        title = h1_tag.text\n",
    "    \n",
    "    # extract categories \n",
    "    categories = []\n",
    "\n",
    "        # Find the <a> tags\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        # Extract the href attribute\n",
    "        href_value = a_tag['href']\n",
    "        \n",
    "        # Use regex to extract the category\n",
    "        match = re.search(r'category/(.*)/', href_value)\n",
    "        \n",
    "        if match:\n",
    "            categories.append(match.group(1))\n",
    "    \n",
    "    # extract channel\n",
    "    channel_tag = soup.find('meta', {'property': 'article:section'})\n",
    "    article_tag = soup.find('article', {'data-channel': True})\n",
    "    channel = article_tag['data-channel']\n",
    "\n",
    "    return len(images), len(links), title, categories, len(categories), channel\n",
    "\n",
    "df[['img_count', 'link_count', 'title', 'categories', 'categories_count', 'channel']] = df['Page content'].apply(attribute_attract).apply(pd.Series)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timedata(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    time_tag = soup.find('time', datetime=True)\n",
    "    if time_tag is None:\n",
    "        return -1\n",
    "    datetime_str = time_tag['datetime']\n",
    "    datetime_obj = datetime.strptime(datetime_str, \"%a, %d %b %Y %H:%M:%S %z\")\n",
    "\n",
    "    day_of_week = datetime_obj.weekday()\n",
    "    year = datetime_obj.year\n",
    "    month = datetime_obj.month\n",
    "    day = datetime_obj.day\n",
    "    hour = datetime_obj.hour\n",
    "    minutes = datetime_obj.minute\n",
    "    sec = datetime_obj.second\n",
    "    timezone = datetime_obj.tzinfo.tzname(datetime_obj)\n",
    "\n",
    "    return day_of_week, year, month, day, hour, minutes, sec, timezone\n",
    "\n",
    "df[['Weekday', 'Year', 'Month', 'Day', 'Hour', 'Minutes', 'Sec', 'Timezone']] = df['Page content'].apply(extract_timedata).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content  \\\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...   \n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...   \n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...   \n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...   \n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...   \n",
      "\n",
      "   img_count  link_count                                              title  \\\n",
      "0          1          22  NASA's Grand Challenge: Stop Asteroids From De...   \n",
      "1          2          18  Google's New Open Source Patent Pledge: We Won...   \n",
      "2          2          11  Ballin': 2014 NFL Draft Picks Get to Choose Th...   \n",
      "3          1          13        Cameraperson Fails Deliver Slapstick Laughs   \n",
      "4         52          16  NFL Star Helps Young Fan Prove Friendship With...   \n",
      "\n",
      "                                          categories  categories_count  \\\n",
      "0  [asteroid, asteroids, challenge, earth, space,...                 7   \n",
      "1  [patent-lawsuit-theater, apps-software, google...                 9   \n",
      "2  [nfl, espn, entertainment, nfl, nfl-draft, spo...                 7   \n",
      "3  [youtube, sports, video, videos-watercooler, w...                 5   \n",
      "4  [nfl, instagram, entertainment, instagram, ins...                 7   \n",
      "\n",
      "         channel  Weekday  Year  Month  Day  Hour  Minutes  Sec Timezone  \n",
      "0          world        2  2013      6   19    15        4   30      UTC  \n",
      "1           tech        3  2013      3   28    17       40   55      UTC  \n",
      "2  entertainment        2  2014      5    7    19       15   20      UTC  \n",
      "3    watercooler        4  2013     10   11     2       26   50      UTC  \n",
      "4  entertainment        3  2014      4   17     3       31   43      UTC  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_author_name(html):\n",
    "    pattern = r'<a href=\"/author/([^/]+)/\">'\n",
    "    match = re.search(pattern, html)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        text = BeautifulSoup(html, 'html.parser')\n",
    "        # find the tag with \"byline basic\" class\n",
    "        tag = text.find(class_='byline basic')\n",
    "        # extract the text from the tag\n",
    "        if tag:\n",
    "            # use strip() to remove leading and trailing spaces\n",
    "            tag = tag.get_text().strip()\n",
    "            # remove the \"By \" or \"by \" substring\n",
    "            tag = re.sub(r'^By ', '', tag)\n",
    "            tag = re.sub(r'^by ', '', tag)\n",
    "            # if len(tag)>=2, use only the first two words\n",
    "            if len(tag.split()) >= 2:\n",
    "                tag = ' '.join(tag.split()[:2])\n",
    "                # for the second word, remove the trailing comma and number\n",
    "                tag = re.sub(r',\\s*\\d+$', '', tag)\n",
    "                # remove content that has number\n",
    "                tag = re.sub(r'\\d+', '', tag)\n",
    "                # remove the trailing --\n",
    "                tag = re.sub(r'--$', '', tag)\n",
    "            return tag\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "df['author_name'] = df['Page content'].apply(extract_author_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content  \\\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...   \n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...   \n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...   \n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...   \n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...   \n",
      "\n",
      "   img_count  link_count                                              title  \\\n",
      "0          1          22  NASA's Grand Challenge: Stop Asteroids From De...   \n",
      "1          2          18  Google's New Open Source Patent Pledge: We Won...   \n",
      "2          2          11  Ballin': 2014 NFL Draft Picks Get to Choose Th...   \n",
      "3          1          13        Cameraperson Fails Deliver Slapstick Laughs   \n",
      "4         52          16  NFL Star Helps Young Fan Prove Friendship With...   \n",
      "\n",
      "                                          categories  categories_count  \\\n",
      "0  [asteroid, asteroids, challenge, earth, space,...                 7   \n",
      "1  [patent-lawsuit-theater, apps-software, google...                 9   \n",
      "2  [nfl, espn, entertainment, nfl, nfl-draft, spo...                 7   \n",
      "3  [youtube, sports, video, videos-watercooler, w...                 5   \n",
      "4  [nfl, instagram, entertainment, instagram, ins...                 7   \n",
      "\n",
      "         channel  Weekday  Year  Month  Day  Hour  Minutes  Sec Timezone  \\\n",
      "0          world        2  2013      6   19    15        4   30      UTC   \n",
      "1           tech        3  2013      3   28    17       40   55      UTC   \n",
      "2  entertainment        2  2014      5    7    19       15   20      UTC   \n",
      "3    watercooler        4  2013     10   11     2       26   50      UTC   \n",
      "4  entertainment        3  2014      4   17     3       31   43      UTC   \n",
      "\n",
      "       author_name  \n",
      "0  Clara Moskowitz  \n",
      "1        christina  \n",
      "2        sam-laird  \n",
      "3        sam-laird  \n",
      "4  connor-finnegan  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe need it, not sure\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "\n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mygodimatomato/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "df['title'] = df['title'].apply(tokenizer_stem_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuations(lst):\n",
    "    punctuations = set(string.punctuation)\n",
    "    new_list = []\n",
    "\n",
    "    for item in lst:\n",
    "        new_item = ''.join([char for char in item if char not in punctuations])\n",
    "        new_list.append(new_item)\n",
    "\n",
    "    return new_list\n",
    "\n",
    "df['title'] = df['title'].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_len'] = df['title'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content  \\\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...   \n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...   \n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...   \n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...   \n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...   \n",
      "\n",
      "   img_count  link_count                                              title  \\\n",
      "0          1          22  [nasa, grand, challenge, stop, asteroid, from,...   \n",
      "1          2          18  [google, new, open, sourc, patent, pledge, we,...   \n",
      "2          2          11  [ballin, nfl, draft, pick, get, choos, their, ...   \n",
      "3          1          13      [cameraperson, fail, deliv, slapstick, laugh]   \n",
      "4         52          16  [nfl, star, help, young, fan, prove, friendshi...   \n",
      "\n",
      "                                          categories  categories_count  \\\n",
      "0  [asteroid, asteroids, challenge, earth, space,...                 7   \n",
      "1  [patent-lawsuit-theater, apps-software, google...                 9   \n",
      "2  [nfl, espn, entertainment, nfl, nfl-draft, spo...                 7   \n",
      "3  [youtube, sports, video, videos-watercooler, w...                 5   \n",
      "4  [nfl, instagram, entertainment, instagram, ins...                 7   \n",
      "\n",
      "         channel  Weekday  Year  Month  Day  Hour  Minutes  Sec Timezone  \\\n",
      "0          world        2  2013      6   19    15        4   30      UTC   \n",
      "1           tech        3  2013      3   28    17       40   55      UTC   \n",
      "2  entertainment        2  2014      5    7    19       15   20      UTC   \n",
      "3    watercooler        4  2013     10   11     2       26   50      UTC   \n",
      "4  entertainment        3  2014      4   17     3       31   43      UTC   \n",
      "\n",
      "       author_name  title_len  \n",
      "0  Clara Moskowitz          8  \n",
      "1        christina         12  \n",
      "2        sam-laird         10  \n",
      "3        sam-laird          5  \n",
      "4  connor-finnegan         10  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../datasets_processed/train_processed_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Id                                       Page content  img_count  \\\n",
      "0  27643  <html><head><div class=\"article-info\"><span cl...          1   \n",
      "1  27644  <html><head><div class=\"article-info\"><span cl...          3   \n",
      "2  27645  <html><head><div class=\"article-info\"><span cl...          2   \n",
      "3  27646  <html><head><div class=\"article-info\"><span cl...          1   \n",
      "4  27647  <html><head><div class=\"article-info\"><span cl...          1   \n",
      "\n",
      "   link_count                                              title  \\\n",
      "0          30  [soccer, star, get, twitter, death, threat, af...   \n",
      "1          13              [googl, glass, get, accessori, store]   \n",
      "2          13   [ouya, game, consol, alreadi, sold, out, amazon]   \n",
      "3          15                  [two, ferns, mock, oscar, nomine]   \n",
      "4          10  [sniper, trailer, look, like, eastwood, may, b...   \n",
      "\n",
      "                                          categories  categories_count  \\\n",
      "0  [soccer, twitter, one-direction, entertainment...                 8   \n",
      "1  [google-glass, gadgets, glass, google, project...                 7   \n",
      "2  [amazon, kickstarter, amazon, amazon-kindle, b...                 6   \n",
      "3  [between-two-ferns, film, academy-awards, osca...                 7   \n",
      "4  [american-sniper, awards, bradley-cooper, clin...                 7   \n",
      "\n",
      "         channel  Weekday    Year  Month   Day  Hour  Minutes   Sec Timezone  \\\n",
      "0  entertainment      0.0  2013.0    9.0   9.0  19.0     47.0   2.0      UTC   \n",
      "1           tech      3.0  2013.0   10.0  31.0   9.0     25.0   2.0      UTC   \n",
      "2       business      1.0  2013.0    6.0  25.0  12.0     54.0  54.0      UTC   \n",
      "3           film      2.0  2013.0    2.0  13.0   3.0     30.0  21.0      UTC   \n",
      "4  entertainment      4.0  2014.0   10.0   3.0   1.0     34.0  54.0      UTC   \n",
      "\n",
      "      author_name  title_len  \n",
      "0       sam-laird         11  \n",
      "1  stan-schroeder          5  \n",
      "2  todd-wasserman          7  \n",
      "3    neha-prakash          5  \n",
      "4     josh-dickey          9  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/test.csv')\n",
    "df[['img_count', 'link_count', 'title', 'categories', 'categories_count', 'channel']] = df['Page content'].apply(attribute_attract).apply(pd.Series)\n",
    "df[['Weekday', 'Year', 'Month', 'Day', 'Hour', 'Minutes', 'Sec', 'Timezone']] = df['Page content'].apply(extract_timedata).apply(pd.Series)\n",
    "df['author_name'] = df['Page content'].apply(extract_author_name)\n",
    "df['title'] = df['title'].apply(tokenizer_stem_nostop)\n",
    "df['title'] = df['title'].apply(remove_punctuations)\n",
    "df['title_len'] = df['title'].apply(len)\n",
    "df.to_csv('../datasets_processed/test_processed_2.csv', index=False)\n",
    "\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygodimatomato",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
