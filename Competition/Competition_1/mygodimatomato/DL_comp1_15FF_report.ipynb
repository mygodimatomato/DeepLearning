{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "詹博允\t112062524\t\n",
    "吳柏諭\t112062585\t\n",
    "李呂元\t112062677\t\n",
    "張博智\t111062704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **feature engineering**\n",
    "\n",
    "---\n",
    "取出新聞的\n",
    "1.   topics(文章標題)\n",
    "2.   len_content()\n",
    "3.   hour(發布時間中的小時)\n",
    "4.   day(發布時間中的星期幾)\n",
    "5.   date(發布時間中的日期)\n",
    "6.   month(發布的月份)\n",
    "7.   year(發布年分)\n",
    "8.   title_bit_count(文章標題之bit數量)\n",
    "9.   author(文章之作者名稱)\n",
    "### **使用features**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "最後採用下列項目作為model's features\n",
    "1.   author(CountVectorizer)\n",
    "2.   topics(CountVectorizer)\n",
    "3.   len_content\n",
    "4.   hour\n",
    "5.   day\n",
    "6.   date\n",
    "7.   month\n",
    "8.   year\n",
    "9.   title_bit_count\n",
    "10.   channel(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../datasets/train.csv')\n",
    "df_test = pd.read_csv ('../datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:]['Page content'].values\n",
    "y_train = df_train.iloc[:]['Popularity'].values\n",
    "y_train[y_train==-1] = 0\n",
    "\n",
    "X_test = df_test.iloc[:]['Page content'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_preprocess(text):\n",
    "    text = re.sub('topics: ', '', text.lower())\n",
    "    text = re.sub(',', ' ,', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_preprocess(text):\n",
    "    text = re.sub('By', '', text)\n",
    "    text = re.sub('by', '', text)\n",
    "    text = re.sub(',', ' ,', text)\n",
    "    text = re.sub(' and ', ' , ', text)\n",
    "    text = re.sub('&', ',', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    article_info = soup.head.find('div', {'class': 'article-info'})\n",
    "    author = article_info.find('span', {'class': 'author_name'})\n",
    "    if author != None:\n",
    "        author = author.get_text()\n",
    "    elif article_info.span != None:\n",
    "        author = article_info.span.string\n",
    "    else:\n",
    "        author = article_info.a.string\n",
    "    author = author_preprocess(author)\n",
    "\n",
    "    topics = soup.find('footer', {'class':'article-topics'}).text\n",
    "    topics = topic_preprocess(topics)\n",
    "\n",
    "    channel = soup.find('article')['data-channel']\n",
    "\n",
    "    try:\n",
    "        date_time = article_info.time['datetime']\n",
    "    except:\n",
    "        date_time = 'Wed, 10 Oct 2014 15:00:43'\n",
    "    \n",
    "    match_obj = re.search('([\\w]+),\\s+([\\d]+)\\s+([\\w]+)\\s+([\\d]+)\\s+([\\d]+):([\\d]+):([\\d]+)', date_time)\n",
    "    day, date, month, year, hour, minute, second = match_obj.groups()\n",
    "    day, month = day.lower(), month.lower()\n",
    "\n",
    "    content = soup.find('section', {'class':'article-content'}).text\n",
    "    len_content = len(content)\n",
    "\n",
    "    h1_tag = soup.find('h1', {'class': 'title'})\n",
    "    title = \"\"\n",
    "    if h1_tag is not None:\n",
    "        title = h1_tag.text\n",
    "    title_bit = len(title)\n",
    "    words = title.split()\n",
    "    title_word_count = len(words)\n",
    "    title_bit_count = title_bit - title_word_count + 1\n",
    "    images = soup.find_all('img')\n",
    "    img_count = len(images)\n",
    "\n",
    "    return author, topics, channel, len_content, hour, day, date, month, year, title_bit_count, img_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_map = {'mon': 1, 'tue': 2, 'wed': 3,\n",
    "           'thu': 4, 'fri': 5, 'sat': 6, 'sun': 7}\n",
    "\n",
    "month_map = {'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "             'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12}\n",
    "\n",
    "df_train = pd.DataFrame(columns=['author', 'topics','channel', 'len_content', 'hour', 'day', 'date', 'month', 'year','title_bit_count', 'img_count'])\n",
    "for idx, x in enumerate(X_train):\n",
    "    df_train.loc[idx] = get_feature(x)\n",
    "df_train['day'] = df_train['day'].map(day_map)\n",
    "df_train['month'] = df_train['month'].map(month_map)\n",
    "df_train['title_bit_count'] = df_train['title_bit_count'].astype(np.int64)\n",
    "\n",
    "df_test = pd.DataFrame(columns=['author', 'topics', 'channel','len_content', 'hour', 'day', 'date', 'month', 'year', 'title_bit_count', 'img_count'])\n",
    "for idx, x in enumerate(X_test):\n",
    "    df_test.loc[idx] = get_feature(x)\n",
    "df_test['day'] = df_test['day'].map(day_map)\n",
    "df_test['month'] = df_test['month'].map(month_map)\n",
    "df_test['title_bit_count'] = df_test['title_bit_count'].astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['title_bit_count'], axis=1, inplace=True)\n",
    "df_test.drop(['title_bit_count'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_author(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    authors = re.split(',', text)\n",
    "    for idx, author in enumerate(authors):\n",
    "        authors[idx] = re.sub(' ', '', author)\n",
    "    return authors\n",
    "\n",
    "# day/topic/author/ 0.574 +- 0.007\n",
    "# day/topic/author/month 0.589 +- 0.007\n",
    "# day/topic/author/month/hour 0.59 +- 0.007\n",
    "# day/topic/author/month/hour/len_content 0.59 +- 0.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = ColumnTransformer(\n",
    "    [('author', CountVectorizer(tokenizer=tokenizer_author, lowercase=False), [0]),\n",
    "     ('topics', CountVectorizer(tokenizer=tokenizer_author, lowercase=False), [1]),\n",
    "     ('channel', CountVectorizer(tokenizer=tokenizer_author, lowercase=False), [2])],\n",
    "    n_jobs=-1,\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vect = ColumnTransformer(\n",
    "#     [\n",
    "#      ('topics', CountVectorizer(tokenizer=tokenizer_author, lowercase=False), [0]),\n",
    "#      ('channel', CountVectorizer(tokenizer=tokenizer_author, lowercase=False), [1])],\n",
    "#     n_jobs=-1,\n",
    "#     remainder='passthrough'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How do you build the classifier?\n",
    "- 將資料通過`ColumnTransformer`，把前處理的資料整合成`vect`。\n",
    "- 我們是使用了三個模型，`LBGMClassifier`/`CatboostClassifier`/`XGBoostClassifier`來做預測。我們原本還有使用到`RandomForrestClassifier`等架構來預測，但是我們發現效果不彰，所以後來沒有特別採用。\n",
    "- 然後再把三個模型的預測結果用`Voteclassifier`來合併投票，他們的權重分別是[0.5,0.35,0.35]。\n",
    "- 至於投票結果依照`roc_auc`輸出，我們是使用`cross_val_score`，然後將valid/train set依照`1:4`的比例做訓練(`cv=5`)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n = 100\n",
    "depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbgm = Pipeline([('vect', vect),\n",
    "                  ('clf', LGBMClassifier(n_estimators=n, \n",
    "                                         max_depth=depth, \n",
    "                                         learning_rate=0.1, \n",
    "                                         random_state=0,\n",
    "                                         num_leaves=(2**(depth-1)), # 2^depth - 1\n",
    "                                         min_data_in_leaf=(2**(depth-4)),\n",
    "                                         n_jobs=-1,\n",
    "                                         delta=0.1))])\n",
    "\n",
    "scores = cross_val_score(estimator=lbgm, X=df_train.values, y=y_train, cv=5, scoring='roc_auc')\n",
    "print('%.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Pipeline([('vect', vect),\n",
    "                    ('clf', CatBoostClassifier(iterations=30, learning_rate=0.2, depth =depth, random_state=0))])\n",
    "scores = cross_val_score(estimator=cat, X=df_train.values, y=y_train, cv=5, scoring='roc_auc')\n",
    "print('%.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = Pipeline([('vect', vect),\n",
    "                    ('clf', XGBClassifier(n_estimators=n, max_depth=10, learning_rate=0.1, random_state=0))])\n",
    "scores = cross_val_score(estimator=xgboost, X=df_train.values, y=y_train, cv=5, scoring='roc_auc')\n",
    "print('%.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "vote = VotingClassifier(estimators=[('lbgm', lbgm), ('cat', cat), ('xgboost', xgboost)], voting='soft', weights=[0.5, 0.35,0.35])\n",
    "scores = cross_val_score(estimator=vote, X=df_train.values, y=y_train, cv=5, scoring='roc_auc')\n",
    "print('%.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote.fit(X=df_train.values, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = vote.predict_proba(df_test.values)[:, 1]\n",
    "result = pd.DataFrame(columns=['Id', 'Popularity'])\n",
    "result['Id'] = np.arange(27643, 27643+len(y_pred))\n",
    "result['Popularity'] = y_pred\n",
    "result.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict\n",
    "popo_version : \n",
    "\tfeature :\n",
    "\t\t'author', 'topics','channel', 'len_content', 'hour', 'day', 'date', 'month', 'year', 'title_bit_count'\n",
    "\tseparate model_acc : \n",
    "\t\tlgbm \t: 0.593 (+/-0.007)\n",
    "\t\tcat  \t: 0.596 (+/-0.006)\n",
    "\t\txgboost : 0.592 (+/-0.006)\n",
    "\tFinal : \t  0.601 (+/-0.007)\n",
    "\tupload : \t  0.59224\n",
    "\n",
    "\n",
    "popo_version : \n",
    "\tfeature : \n",
    "\t\t'author', 'topics','channel', 'len_content', 'hour', 'day', 'date', 'month', 'year','title_bit_count', 'img_count'\n",
    "\tseparate model_acc :\n",
    "\t\tlgbm \t: 0.592 (+/-0.006)\n",
    "\t\tcat\t\t: 0.595 (+/-0.010)\n",
    "\t\txgboost\t: 0.592 (+/-0.005)\n",
    "\tFinal : \t  0.601 (+/-0.007)\n",
    "\tupload : \t  0.59127\n",
    "\n",
    "popo_version :\n",
    "\tfeature :\n",
    "\t\t\n",
    "\tseparate model_acc :\n",
    "\t\tlgbm\t: 0.593 (+/-0.008)\n",
    "\t\tcat \t: 0.594 (+/-0.009)\n",
    "\t\txgboost : 0.590 (+/-0.006)\n",
    "\tFinal :\t\t  0.599 (+/-0.008)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion #\n",
    "1. 選對 model 比努力更重要 -> lightGBM, XGBoost 很好用！\n",
    "2. 發現日期很重要 -> 只用日期去 train acc 就 58.5%\n",
    "3. 如何讓 model 有一般性 -> 因為發現最後在比的是誰的 model 比較 generalize\n",
    "4. 對資料分析有更深的理解 -> 在 year 這筆 feature, 因為 bias 不大我以為對 prediction 的影響不大, 但是後來發現其 prediction weight 其實很大\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
