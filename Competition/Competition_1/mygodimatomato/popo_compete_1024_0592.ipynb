{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../datasets/train.csv')\n",
    "df_test = pd.read_csv ('../datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:]['Page content'].values\n",
    "y_train = df_train.iloc[:]['Popularity'].values\n",
    "y_train[y_train==-1] = 0\n",
    "\n",
    "X_test = df_test.iloc[:]['Page content'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_preprocess(text):\n",
    "    text = re.sub('topics: ', '', text.lower())\n",
    "    text = re.sub(',', ' ,', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_preprocess(text):\n",
    "    text = re.sub('By', '', text)\n",
    "    text = re.sub('by', '', text)\n",
    "    text = re.sub(',', ' ,', text)\n",
    "    text = re.sub(' and ', ' , ', text)\n",
    "    text = re.sub('&', ',', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    article_info = soup.head.find('div', {'class': 'article-info'})\n",
    "    author = article_info.find('span', {'class': 'author_name'})\n",
    "    if author != None:\n",
    "        author = author.get_text()\n",
    "    elif article_info.span != None:\n",
    "        author = article_info.span.string\n",
    "    else:\n",
    "        author = article_info.a.string\n",
    "    author = author_preprocess(author)\n",
    "\n",
    "    topics = soup.find('footer', {'class':'article-topics'}).text\n",
    "    topics = topic_preprocess(topics)\n",
    "\n",
    "    channel = soup.find('article')['data-channel']\n",
    "\n",
    "    try:\n",
    "        date_time = article_info.time['datetime']\n",
    "    except:\n",
    "        date_time = 'Wed, 10 Oct 2014 15:00:43'\n",
    "    \n",
    "    match_obj = re.search('([\\w]+),\\s+([\\d]+)\\s+([\\w]+)\\s+([\\d]+)\\s+([\\d]+):([\\d]+):([\\d]+)', date_time)\n",
    "    day, date, month, year, hour, minute, second = match_obj.groups()\n",
    "    day, month = day.lower(), month.lower()\n",
    "\n",
    "    content = soup.find('section', {'class':'article-content'}).text\n",
    "    len_content = len(content)\n",
    "\n",
    "    h1_tag = soup.find('h1', {'class': 'title'})\n",
    "    title = \"\"\n",
    "    if h1_tag is not None:\n",
    "        title = h1_tag.text\n",
    "    title_bit = len(title)\n",
    "    words = title.split()\n",
    "    title_word_count = len(words)\n",
    "    title_bit_count = title_bit - title_word_count + 1\n",
    "    images = soup.find_all('img')\n",
    "    img_count = len(images)\n",
    "\n",
    "    return author, topics, channel, len_content, hour, day, date, month, year, title_bit_count, img_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_map = {'mon': 1, 'tue': 2, 'wed': 3,\n",
    "           'thu': 4, 'fri': 5, 'sat': 6, 'sun': 7}\n",
    "\n",
    "month_map = {'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "             'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12}\n",
    "\n",
    "df_train = pd.DataFrame(columns=['author', 'topics','channel', 'len_content', 'hour', 'day', 'date', 'month', 'year','title_bit_count', 'img_count'])\n",
    "for idx, x in enumerate(X_train):\n",
    "    df_train.loc[idx] = get_feature(x)\n",
    "df_train['day'] = df_train['day'].map(day_map)\n",
    "df_train['month'] = df_train['month'].map(month_map)\n",
    "df_train['title_bit_count'] = df_train['title_bit_count'].astype(np.int64)\n",
    "\n",
    "df_test = pd.DataFrame(columns=['author', 'topics', 'channel','len_content', 'hour', 'day', 'date', 'month', 'year', 'title_bit_count', 'img_count'])\n",
    "for idx, x in enumerate(X_test):\n",
    "    df_test.loc[idx] = get_feature(x)\n",
    "df_test['day'] = df_test['day'].map(day_map)\n",
    "df_test['month'] = df_test['month'].map(month_map)\n",
    "df_test['title_bit_count'] = df_test['title_bit_count'].astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['img_count', 'title_bit_count', 'author'], axis=1, inplace=True)\n",
    "df_test.drop(['img_count', 'title_bit_count', 'author'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_author(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    authors = re.split(',', text)\n",
    "    for idx, author in enumerate(authors):\n",
    "        authors[idx] = re.sub(' ', '', author)\n",
    "    return authors\n",
    "\n",
    "# day/topic/author/ 0.574 +- 0.007\n",
    "# day/topic/author/month 0.589 +- 0.007\n",
    "# day/topic/author/month/hour 0.59 +- 0.007\n",
    "# day/topic/author/month/hour/len_content 0.59 +- 0.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vect = ColumnTransformer(\n",
    "#     [('author', CountVectorizer(tokenizer=tokenizer_author, lowercase=False), [0]),\n",
    "#      ('topics', CountVectorizer(tokenizer=tokenizer_author, lowercase=False), [1]),\n",
    "#      ('channel', CountVectorizer(tokenizer=tokenizer_author, lowercase=False), [2])],\n",
    "#     n_jobs=-1,\n",
    "#     remainder='passthrough'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = ColumnTransformer(\n",
    "    [\n",
    "     ('topics', CountVectorizer(tokenizer=tokenizer_author, lowercase=False), [0]),\n",
    "     ('channel', CountVectorizer(tokenizer=tokenizer_author, lowercase=False), [1])],\n",
    "    n_jobs=-1,\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n = 100\n",
    "depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbgm = Pipeline([('vect', vect),\n",
    "                  ('clf', LGBMClassifier(n_estimators=n, \n",
    "                                         max_depth=depth, \n",
    "                                         learning_rate=0.1, \n",
    "                                         random_state=0,\n",
    "                                         num_leaves=(2**(depth-1)), # 2^depth - 1\n",
    "                                         min_data_in_leaf=(2**(depth-4)),\n",
    "                                         n_jobs=-1,\n",
    "                                         delta=0.1))])\n",
    "\n",
    "scores = cross_val_score(estimator=lbgm, X=df_train.values, y=y_train, cv=5, scoring='roc_auc')\n",
    "print('%.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Pipeline([('vect', vect),\n",
    "                    ('clf', CatBoostClassifier(iterations=30, learning_rate=0.2, depth =depth, random_state=0))])\n",
    "scores = cross_val_score(estimator=cat, X=df_train.values, y=y_train, cv=5, scoring='roc_auc')\n",
    "print('%.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = Pipeline([('vect', vect),\n",
    "                    ('clf', XGBClassifier(n_estimators=n, max_depth=10, learning_rate=0.1, random_state=0))])\n",
    "scores = cross_val_score(estimator=xgboost, X=df_train.values, y=y_train, cv=5, scoring='roc_auc')\n",
    "print('%.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "vote = VotingClassifier(estimators=[('lbgm', lbgm), ('cat', cat), ('xgboost', xgboost)], voting='soft', weights=[0.5, 0.35,0.35])\n",
    "scores = cross_val_score(estimator=vote, X=df_train.values, y=y_train, cv=5, scoring='roc_auc')\n",
    "print('%.3f (+/-%.3f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote.fit(X=df_train.values, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = vote.predict_proba(df_test.values)[:, 1]\n",
    "result = pd.DataFrame(columns=['Id', 'Popularity'])\n",
    "result['Id'] = np.arange(27643, 27643+len(y_pred))\n",
    "result['Popularity'] = y_pred\n",
    "result.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
