{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils, datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import pickle\n",
    "import csv\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "if not os.path.exists(\"lab11_1_lib.py\"):\n",
    "    urllib.request.urlretrieve(\"https://nthu-datalab.github.io/ml/labs/11-1_CNN/lab11_1_lib.py\", \"lab11_1_lib.py\")\n",
    "\n",
    "from lab11_1_lib import draw_timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_image: (60000, 28, 28)\n",
      "shape of train_label: (60000,)\n"
     ]
    }
   ],
   "source": [
    "# Download and prepare the MNIST dataset\n",
    "(train_image, train_label), (test_image, test_label) = datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_image, test_image = train_image / 255.0, test_image / 255.0\n",
    "print('shape of train_image:', train_image.shape)\n",
    "print('shape of train_label:', train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# flating the training data for dense layers\n",
    "train_image_1 = train_image.reshape((60000, -1))\n",
    "test_image_1 = test_image.reshape((10000, -1))\n",
    "print(train_image_1.shape)\n",
    "print(test_image_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = models.Sequential()\n",
    "model_1.add(layers.Dense(10, activation='softmax',input_shape=(784,)))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.7286 - accuracy: 0.8112\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3160 - accuracy: 0.9123\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2864 - accuracy: 0.9202\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2710 - accuracy: 0.9234\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2629 - accuracy: 0.9272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ab383926a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model and train it for 5 epochs\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1.fit(train_image_1, train_label, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy : 0.9241\n"
     ]
    }
   ],
   "source": [
    "_, test_acc_1 = model_1.evaluate(test_image_1, test_label, verbose=0)\n",
    "print('Testing Accuracy : %.4f'%test_acc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshaping the training data to 3 dimensions\n",
    "train_image_2 = train_image.reshape((60000, 28, 28, 1))\n",
    "test_image_2 = test_image.reshape((10000, 28, 28, 1))\n",
    "print(train_image_2.shape)\n",
    "print(test_image_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 6 lines of code below define the convolutional base using a common pattern: a stack of Conv2D and MaxPooling2D layers.\n",
    "model_2 = models.Sequential()\n",
    "model_2.add(layers.Conv2D(32, (3, 3), strides=(1,1), padding='valid', activation='relu', input_shape=(28, 28, 1)))\n",
    "model_2.add(layers.MaxPooling2D((2, 2)))\n",
    "model_2.add(layers.Conv2D(64, (3, 3), strides=(1,1), padding='valid', activation='relu'))\n",
    "model_2.add(layers.MaxPooling2D((2, 2)))\n",
    "model_2.add(layers.Conv2D(64, (3, 3), strides=(1,1), padding='valid', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.add(layers.Flatten())\n",
    "model_2.add(layers.Dense(64, activation='relu'))\n",
    "model_2.add(layers.Dropout(0.5))\n",
    "model_2.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 13s 5ms/step - loss: 0.5255 - accuracy: 0.8293\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0913 - accuracy: 0.9750\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0595 - accuracy: 0.9826\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0455 - accuracy: 0.9871\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0396 - accuracy: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ab37f10cd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_2.fit(train_image_2, train_label, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy : 0.9916\n"
     ]
    }
   ],
   "source": [
    "_, test_acc_2 = model_2.evaluate(test_image_2, test_label, verbose=0)\n",
    "print('Testing Accuracy : %.4f'%test_acc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 14s 0us/step\n",
      "X_train shape: (50000, 32, 32, 3)\n",
      "Y_train shape: (50000, 10)\n",
      "X_test shape: (10000, 32, 32, 3)\n",
      "Y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()\n",
    "\n",
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# convert class vectors to binary vectors\n",
    "Y_train = utils.to_categorical(y_train)\n",
    "Y_test = utils.to_categorical(y_test)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transform a 3-channel image into one channel\n",
    "def grayscale(data, dtype='float32'):\n",
    "    # luma coding weighted average in video systems\n",
    "    r = np.asarray(.3, dtype=dtype)\n",
    "    g = np.asarray(.59, dtype=dtype)\n",
    "    b = np.asarray(.11, dtype=dtype)\n",
    "    rst = r * data[:, :, :, 0] + g * data[:, :, :, 1] + b * data[:, :, :, 2]\n",
    "    # add channel dimension\n",
    "    rst = np.expand_dims(rst, axis=3)\n",
    "    return rst\n",
    "\n",
    "X_train_gray = grayscale(X_train)\n",
    "X_test_gray = grayscale(X_test)\n",
    "\n",
    "# plot a randomly chosen image\n",
    "img = round(np.random.rand() * X_train.shape[0])\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X_train[img], interpolation='none')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(\n",
    "    X_train_gray[img, :, :, 0], cmap=plt.get_cmap('gray'), interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHOGfeat(image,\n",
    "               stride=8,\n",
    "               orientations=8,\n",
    "               pixels_per_cell=(8, 8),\n",
    "               cells_per_block=(2, 2)):\n",
    "    cx, cy = pixels_per_cell\n",
    "    bx, by = cells_per_block\n",
    "    sx, sy, sz = image.shape\n",
    "    n_cellsx = int(np.floor(sx // cx))  # number of cells in x\n",
    "    n_cellsy = int(np.floor(sy // cy))  # number of cells in y\n",
    "    n_blocksx = (n_cellsx - bx) + 1\n",
    "    n_blocksy = (n_cellsy - by) + 1\n",
    "    gx = np.zeros((sx, sy), dtype=np.double)\n",
    "    gy = np.zeros((sx, sy), dtype=np.double)\n",
    "    eps = 1e-5\n",
    "    grad = np.zeros((sx, sy, 2), dtype=np.double)\n",
    "    for i in range(1, sx - 1):\n",
    "        for j in range(1, sy - 1):\n",
    "            gx[i, j] = image[i, j - 1] - image[i, j + 1]\n",
    "            gy[i, j] = image[i + 1, j] - image[i - 1, j]\n",
    "            grad[i, j, 0] = np.arctan(gy[i, j] / (gx[i, j] + eps)) * 180 / math.pi\n",
    "            if gx[i, j] < 0:\n",
    "                grad[i, j, 0] += 180\n",
    "            grad[i, j, 0] = (grad[i, j, 0] + 360) % 360\n",
    "            grad[i, j, 1] = np.sqrt(gy[i, j] ** 2 + gx[i, j] ** 2)\n",
    "    normalised_blocks = np.zeros((n_blocksy, n_blocksx, by * bx * orientations))\n",
    "    for y in range(n_blocksy):\n",
    "        for x in range(n_blocksx):\n",
    "            block = grad[y * stride:y * stride + 16, x * stride:x * stride + 16]\n",
    "            hist_block = np.zeros(32, dtype=np.double)\n",
    "            eps = 1e-5\n",
    "            for k in range(by):\n",
    "                for m in range(bx):\n",
    "                    cell = block[k * 8:(k + 1) * 8, m * 8:(m + 1) * 8]\n",
    "                    hist_cell = np.zeros(8, dtype=np.double)\n",
    "                    for i in range(cy):\n",
    "                        for j in range(cx):\n",
    "                            n = int(cell[i, j, 0] / 45)\n",
    "                            hist_cell[n] += cell[i, j, 1]\n",
    "                    hist_block[(k * bx + m) * orientations:(k * bx + m + 1) * orientations] = hist_cell[:]\n",
    "            normalised_blocks[y, x, :] = hist_block / np.sqrt(\n",
    "                hist_block.sum() ** 2 + eps)\n",
    "    return normalised_blocks.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will take some minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [10:00<00:00, 83.32it/s]\n",
      "100%|██████████| 10000/10000 [02:00<00:00, 83.10it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_hog = []\n",
    "X_test_hog = []\n",
    "\n",
    "print('This will take some minutes.')\n",
    "\n",
    "for img in tqdm(X_train_gray):\n",
    "    img_hog = getHOGfeat(img)\n",
    "    X_train_hog.append(img_hog)\n",
    "\n",
    "for img in tqdm(X_test_gray):\n",
    "    img_hog = getHOGfeat(img)\n",
    "    X_test_hog.append(img_hog)\n",
    "    \n",
    "X_train_hog_array = np.asarray(X_train_hog)\n",
    "X_test_hog_array = np.asarray(X_test_hog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNN]\n",
      "Misclassified samples: 5334\n",
      "Accuracy: 0.47\n"
     ]
    }
   ],
   "source": [
    "# # KNN\n",
    "# from sklearn.neighbors import KNeighborsClassifier \n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # p=2 and metric='minkowski' means the Euclidean Distance\n",
    "# knn = KNeighborsClassifier(n_neighbors=11, p=2, metric='minkowski')\n",
    "\n",
    "# knn.fit(X_train_hog_array, y_train.ravel())\n",
    "# y_pred = knn.predict(X_test_hog_array)\n",
    "# print('[KNN]')\n",
    "# print('Misclassified samples: %d' % (y_test.ravel() != y_pred).sum())\n",
    "# print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will take some minutes.\n",
      "[Linear SVC]\n",
      "Misclassified samples: 4940\n",
      "Accuracy: 0.51\n",
      "649.32 sec.\n"
     ]
    }
   ],
   "source": [
    "# # SVM\n",
    "# from sklearn.svm import SVC \n",
    "\n",
    "# print('This will take some minutes.')\n",
    "# start_time = time.time()\n",
    "\n",
    "# # C is the hyperparameter for the error penalty term\n",
    "# # gamma is the hyperparameter for the rbf kernel\n",
    "# svm_linear = SVC(kernel='linear', random_state=0, gamma=0.2, C=10.0)\n",
    "\n",
    "# svm_linear.fit(X_train_hog_array, y_train.ravel())\n",
    "# y_pred = svm_linear.predict(X_test_hog_array)\n",
    "# print('[Linear SVC]')\n",
    "# print('Misclassified samples: %d' % (y_test.ravel() != y_pred).sum())\n",
    "# print('Accuracy: %.2f' % accuracy_score(y_test.ravel(), y_pred))\n",
    "\n",
    "# print('{:.2f} sec.'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 64)        4864      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 384)               1573248   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 192)               73920     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1930      \n",
      "=================================================================\n",
      "Total params: 1,756,938\n",
      "Trainable params: 1,756,682\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3 = models.Sequential()\n",
    "\n",
    "#The 6 lines of code below define the convolutional base using a common pattern: a stack of Conv2D and MaxPooling2D layers.\n",
    "model_3.add(layers.Conv2D(64, (5, 5), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "model_3.add(layers.MaxPool2D(pool_size=3,strides=2,padding='same'))\n",
    "model_3.add(layers.BatchNormalization())\n",
    "model_3.add(layers.Conv2D(64, (5, 5), padding='same', activation='relu'))\n",
    "model_3.add(layers.MaxPool2D(pool_size=3,strides=2,padding='same'))\n",
    "model_3.add(layers.BatchNormalization())\n",
    "                \n",
    "model_3.add(layers.Flatten())\n",
    "model_3.add(layers.Dense(384, activation='relu'))\n",
    "model_3.add(layers.Dropout(0.5))\n",
    "model_3.add(layers.Dense(192, activation='relu'))\n",
    "model_3.add(layers.Dense(10, activation='softmax'))\n",
    "model_3.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.9082 - accuracy: 0.3502 - val_loss: 1.2610 - val_accuracy: 0.5595\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1588 - accuracy: 0.5894 - val_loss: 1.0036 - val_accuracy: 0.6679\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.9484 - accuracy: 0.6670 - val_loss: 1.0161 - val_accuracy: 0.6548\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8278 - accuracy: 0.7153 - val_loss: 0.9192 - val_accuracy: 0.6812\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7306 - accuracy: 0.7457 - val_loss: 0.7950 - val_accuracy: 0.7246\n",
      "Testing Accuracy : 0.7246\n"
     ]
    }
   ],
   "source": [
    "model_3.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), verbose=1)\n",
    "_, test_acc_3 = model_3.evaluate(X_test, y_test, verbose=0)\n",
    "print('Testing Accuracy : %.4f'%test_acc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 5) (200,)\n"
     ]
    }
   ],
   "source": [
    "# number of samples\n",
    "n_samples = 200\n",
    "\n",
    "# an array with shape (n_samples, 5)\n",
    "raw_data_a = np.random.rand(n_samples, 5)\n",
    "# a list with length of n_samples from 0 to n_samples-1\n",
    "raw_data_b = np.arange(n_samples)\n",
    "print(raw_data_a.shape, raw_data_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tells the dataset that each row of raw_data_a is corresponding to each element of raw_data_b\n",
    "raw_dataset = tf.data.Dataset.from_tensor_slices((raw_data_a, raw_data_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  0 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.43539956, 0.00506565, 0.98644897, 0.78440606, 0.29490015])>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "Batch  1 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.48580055, 0.794433  , 0.16131837, 0.20065087, 0.34292575])>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "Batch  2 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.18233817, 0.92311146, 0.40593751, 0.31472856, 0.29257028])>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "Batch  3 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.13467984, 0.87299602, 0.65002907, 0.59365984, 0.49341016])>, <tf.Tensor: shape=(), dtype=int64, numpy=3>)\n",
      "Batch  4 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.85498532, 0.07293431, 0.93044152, 0.75865421, 0.18699617])>, <tf.Tensor: shape=(), dtype=int64, numpy=4>)\n",
      "Batch  5 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.14014767, 0.06128635, 0.31925296, 0.54753811, 0.36803201])>, <tf.Tensor: shape=(), dtype=int64, numpy=5>)\n",
      "Batch  6 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.31273436, 0.24834609, 0.8942605 , 0.14981866, 0.90572103])>, <tf.Tensor: shape=(), dtype=int64, numpy=6>)\n",
      "Batch  7 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.22828511, 0.99919365, 0.12225093, 0.03513537, 0.98320107])>, <tf.Tensor: shape=(), dtype=int64, numpy=7>)\n"
     ]
    }
   ],
   "source": [
    "# Here, we print the first 8 batches.\n",
    "for i,elem in enumerate(raw_dataset):\n",
    "    print(\"Batch \", i, \", b are \", elem)\n",
    "    if i==7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  0 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.43539956, 0.00506565, 0.98644897, 0.78440606, 0.29490015])>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "Batch  1 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.48580055, 0.794433  , 0.16131837, 0.20065087, 0.34292575])>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "Batch  2 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.18233817, 0.92311146, 0.40593751, 0.31472856, 0.29257028])>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "Batch  3 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.13467984, 0.87299602, 0.65002907, 0.59365984, 0.49341016])>, <tf.Tensor: shape=(), dtype=int64, numpy=3>)\n",
      "Batch  4 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.85498532, 0.07293431, 0.93044152, 0.75865421, 0.18699617])>, <tf.Tensor: shape=(), dtype=int64, numpy=4>)\n",
      "Batch  5 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.14014767, 0.06128635, 0.31925296, 0.54753811, 0.36803201])>, <tf.Tensor: shape=(), dtype=int64, numpy=5>)\n",
      "Batch  6 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.31273436, 0.24834609, 0.8942605 , 0.14981866, 0.90572103])>, <tf.Tensor: shape=(), dtype=int64, numpy=6>)\n",
      "Batch  7 , b are  (<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.22828511, 0.99919365, 0.12225093, 0.03513537, 0.98320107])>, <tf.Tensor: shape=(), dtype=int64, numpy=7>)\n"
     ]
    }
   ],
   "source": [
    "# Here, we print the first 8 batches.\n",
    "it = iter(raw_dataset)\n",
    "for i in range(8):\n",
    "    print(\"Batch \", i, \", b are \", next(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(one_row_a, one_b):\n",
    "    \"\"\"\n",
    "        Input: one slice of the dataset\n",
    "        Output: modified slice\n",
    "    \"\"\"\n",
    "    # Do some data preprocessing, you can also input filenames and load data in here\n",
    "    # Here, we transform each row of raw_data_a to its sum and mean\n",
    "    one_row_a = [tf.reduce_sum(one_row_a), tf.reduce_mean(one_row_a)]\n",
    "\n",
    "    return one_row_a, one_b\n",
    "\n",
    "raw_dataset = raw_dataset.map(preprocess_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  0 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([2.50622039, 0.50124408])>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "Batch  1 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([1.98512853, 0.39702571])>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "Batch  2 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([2.11868597, 0.42373719])>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "Batch  3 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([2.74477493, 0.54895499])>, <tf.Tensor: shape=(), dtype=int64, numpy=3>)\n",
      "Batch  4 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([2.80401154, 0.56080231])>, <tf.Tensor: shape=(), dtype=int64, numpy=4>)\n",
      "Batch  5 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([1.43625711, 0.28725142])>, <tf.Tensor: shape=(), dtype=int64, numpy=5>)\n",
      "Batch  6 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([2.51088065, 0.50217613])>, <tf.Tensor: shape=(), dtype=int64, numpy=6>)\n",
      "Batch  7 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([2.36806613, 0.47361323])>, <tf.Tensor: shape=(), dtype=int64, numpy=7>)\n"
     ]
    }
   ],
   "source": [
    "it = iter(raw_dataset)\n",
    "for i in range(8):\n",
    "    print(\"Batch \", i, \", b are \", next(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = raw_dataset.shuffle(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  0 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([1.06353583, 0.21270717])>, <tf.Tensor: shape=(), dtype=int64, numpy=12>)\n",
      "Batch  1 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([1.98512853, 0.39702571])>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "Batch  2 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([1.59853471, 0.31970694])>, <tf.Tensor: shape=(), dtype=int64, numpy=9>)\n",
      "Batch  3 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([2.61269587, 0.52253917])>, <tf.Tensor: shape=(), dtype=int64, numpy=13>)\n",
      "Batch  4 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([3.20837076, 0.64167415])>, <tf.Tensor: shape=(), dtype=int64, numpy=8>)\n",
      "Batch  5 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([2.11868597, 0.42373719])>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "Batch  6 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([2.74477493, 0.54895499])>, <tf.Tensor: shape=(), dtype=int64, numpy=3>)\n",
      "Batch  7 , b are  (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([2.23477093, 0.44695419])>, <tf.Tensor: shape=(), dtype=int64, numpy=18>)\n",
      "\n",
      "The order of the first 8 shuffle from [0, 1, 2, 3, 4, 5, 6, 7] to  [12, 1, 9, 13, 8, 2, 3, 18]\n"
     ]
    }
   ],
   "source": [
    "idxs = []\n",
    "for i,elem in enumerate(dataset):\n",
    "    print(\"Batch \", i, \", b are \", elem)\n",
    "    idxs.append(elem[1].numpy())\n",
    "    if i==7:\n",
    "        break\n",
    "        \n",
    "print(\"\\nThe order of the first 8 shuffle from [0, 1, 2, 3, 4, 5, 6, 7] to \",idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
