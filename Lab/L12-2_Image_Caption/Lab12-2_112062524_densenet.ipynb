{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 18:05:30.552756: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 18:05:33.862894: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-11-30 18:05:33.864043: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-11-30 18:05:33.875528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2023-11-30 18:05:33.875799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:82:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2023-11-30 18:05:33.876064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: \n",
      "pciBusID: 0000:85:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2023-11-30 18:05:33.876326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: \n",
      "pciBusID: 0000:86:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2023-11-30 18:05:33.876350: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-11-30 18:05:33.879254: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-11-30 18:05:33.879307: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-11-30 18:05:33.881043: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-11-30 18:05:33.881386: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-11-30 18:05:33.883178: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-11-30 18:05:33.884013: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-11-30 18:05:33.884278: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-11-30 18:05:33.886150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2023-11-30 18:05:33.894352: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-11-30 18:05:33.894686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:85:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2023-11-30 18:05:33.894715: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-11-30 18:05:33.894741: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-11-30 18:05:33.894763: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-11-30 18:05:33.894783: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-11-30 18:05:33.894804: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-11-30 18:05:33.894824: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-11-30 18:05:33.894844: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcu"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[2], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sparse.so.11\n",
      "2023-11-30 18:05:33.894865: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-11-30 18:05:33.895362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 2\n",
      "2023-11-30 18:05:33.895401: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-11-30 18:05:34.556897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-11-30 18:05:34.556942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      2 \n",
      "2023-11-30 18:05:34.556951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 2:   N \n",
      "2023-11-30 18:05:34.557954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14957 MB memory) -> physical GPU (device: 2, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)\n"
     ]
    }
   ],
   "source": [
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, AveragePooling2D, BatchNormalization, Activation, Concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import DenseNet121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from './dataset/words_captcha/spec_train_val.txt'\n",
    "import glob\n",
    "\n",
    "img_list = []\n",
    "answer_list = []\n",
    "\n",
    "with open('./dataset/words_captcha/spec_train_val.txt', 'r') as f:\n",
    "  for line in f:\n",
    "    image, answer = line.strip().split()\n",
    "    answer_list.append('<start> ' + ' '.join(answer) + ' <end>')\n",
    "    img_list.append('./dataset/words_captcha/' + image+'.png')\n",
    "\n",
    "# so the words_captcha has some image that is not list in  spec_train_val.txt, \n",
    "# we need to add these image to the img_list\n",
    "tmp = set(glob.glob(f'./dataset/words_captcha/*.png')) - set(img_list)\n",
    "# sort the tmp \n",
    "tmp = sorted(tmp)\n",
    "img_list += tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and tokenize the captions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<unk>\", \n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(answer_list)\n",
    "answer_seqs = tokenizer.texts_to_sequences(answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenized vectors\n",
    "answer_seqs = tokenizer.texts_to_sequences(answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions\n",
    "answer_vector = tf.keras.preprocessing.sequence.pad_sequences(answer_seqs, padding='post')\n",
    "max_length = calc_max_length(answer_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> t h u s <end>', '<start> w w w <end>', '<start> t i e d <end>', '<start> i d s <end>', '<start> j a m <end>', '<start> z o o <end>', '<start> a p p l e <end>', '<start> b i g <end>', '<start> l o t <end>', '<start> a b o v e <end>']\n",
      "[[ 2  9 18 17  6  3  0]\n",
      " [ 2 24 24 24  3  0  0]\n",
      " [ 2  9  8  4 13  3  0]\n",
      " [ 2  8 13  6  3  0  0]\n",
      " [ 2 26  5 16  3  0  0]\n",
      " [ 2 28  7  7  3  0  0]\n",
      " [ 2  5 15 15 11  4  3]\n",
      " [ 2 20  8 19  3  0  0]\n",
      " [ 2 11  7  9  3  0  0]\n",
      " [ 2  5 20  7 25  4  3]]\n"
     ]
    }
   ],
   "source": [
    "print(answer_list[:10])\n",
    "print(answer_vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and testing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the first 100,000 images as training data, the next 20,000 as validation data, and the rest (final 20,000) as testing data.\n",
    "img_train, img_valid, img_test = img_list[:100000], img_list[100000:120000], img_list[120000:]\n",
    "answer_train, answer_valid = answer_vector[:100000], answer_vector[100000:120000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 20000, 20000, 100000, 20000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_train), len(img_valid), len(img_test), len(answer_train), len(answer_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "BUFFER_SIZE = 5000\n",
    "LEARNING_RATE = 1e-4\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_steps = len(img_train) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Image.open(img_list[2])\n",
    "# width, height = img.size\n",
    "# print(\"Width:\", width, \"Height:\", height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(img_name, cap):\n",
    "    img = tf.io.read_file(img_name)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = img/255*2-1\n",
    "    img = tf.image.resize(img, (160, 300))\n",
    "    return img, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_training = tf.data.Dataset.from_tensor_slices((img_train, answer_train))\n",
    "dataset_training = dataset_training.map(lambda item1, item2: tf.numpy_function(\n",
    "                    map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "                    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset_training = dataset_training.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_valid = tf.data.Dataset.from_tensor_slices((img_valid, answer_valid))\n",
    "dataset_valid = dataset_valid.map(lambda item1, item2: tf.numpy_function(\n",
    "                    map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "                    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset_valid = dataset_valid.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train = tf.data.Dataset.from_tensor_slices((img_train, answer_train))\\\n",
    "#                                 .map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "#                                 .shuffle(BUFFER_SIZE)\\\n",
    "#                                 .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "#                                 .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# dataset_valid = tf.data.Dataset.from_tensor_slices((img_valid, answer_valid))\\\n",
    "#                                 .map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "#                                 .shuffle(BUFFER_SIZE)\\\n",
    "#                                 .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "#                                 .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features = DenseNet121(include_top=False, weights='imagenet', input_shape=(160, 300, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  \n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/densenet121_v4\"\n",
    "ckpt = tf.train.Checkpoint(model=extract_features,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "    hidden = decoder.reset_state(batch_size=BATCH_SIZE)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        image_features = extract_features(img_tensor, True)\n",
    "        image_features = tf.reshape(image_features, (image_features.shape[0], -1, image_features.shape[3]))\n",
    "        features = encoder(image_features)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    # trainable_variables = model.trainable_variables\n",
    "    # trainable_variables = encoder.trainable_variables + decoder.trainable_variables \n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables + extract_features.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_tensor):\n",
    "    batch_size = img_tensor.shape[0]\n",
    "    dec_input = tf.expand_dims(\n",
    "        [tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "    features = extract_features(img_tensor)\n",
    "    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "\n",
    "    result = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    for _ in range(max_length):\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions, axis=1).numpy()\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "        result = tf.concat([result, predicted_id.reshape((batch_size, 1))], axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(segs):\n",
    "    result_list = []\n",
    "    for seq in segs:\n",
    "        result = ''\n",
    "        for s in seq[1:]:\n",
    "            if s == tokenizer.word_index['<end>']:\n",
    "                break\n",
    "            result += tokenizer.index_word[s]\n",
    "        result_list.append(result)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset_valid):\n",
    "    sample_count = 0\n",
    "    correct_count = 0\n",
    "    for img_tensor, target in dataset_valid:\n",
    "        pred_list = postprocess(predict(img_tensor).numpy())\n",
    "        real_list = postprocess(target.numpy())\n",
    "\n",
    "        for pred, real in zip(pred_list, real_list):\n",
    "            sample_count += 1\n",
    "            if pred == real:\n",
    "                correct_count += 1\n",
    "    print(f\"sample_count: {sample_count}, correct_count: {correct_count}\")\n",
    "    return correct_count / sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 18:06:02.485964: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-11-30 18:06:02.719237: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-11-30 18:06:02.753226: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "Epoch  2: 100%|██████████| 2000/2000 [10:12<00:00,  3.27it/s, loss=0.257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_count: 20000, correct_count: 19152\n",
      "Validation accuracy: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 2000/2000 [09:48<00:00,  3.40it/s, loss=0.0156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_count: 20000, correct_count: 19555\n",
      "Validation accuracy: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4: 100%|██████████| 2000/2000 [09:49<00:00,  3.39it/s, loss=0.0111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_count: 20000, correct_count: 17642\n",
      "Validation accuracy: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5: 100%|██████████| 2000/2000 [09:48<00:00,  3.40it/s, loss=0.00563]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     loss_plot\u001b[39m.\u001b[39mappend(total_loss \u001b[39m/\u001b[39m num_steps)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     ckpt_manager\u001b[39m.\u001b[39msave()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     score \u001b[39m=\u001b[39m evaluate(dataset_valid)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation accuracy: \u001b[39m\u001b[39m{\u001b[39;00mscore\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTime taken for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sec\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(EPOCHS \u001b[39m-\u001b[39m start_epoch, time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start))\n",
      "\u001b[1;32m/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb Cell 37\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m correct_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m img_tensor, target \u001b[39min\u001b[39;00m dataset_valid:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     pred_list \u001b[39m=\u001b[39m postprocess(predict(img_tensor)\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     real_list \u001b[39m=\u001b[39m postprocess(target\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m pred, real \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(pred_list, real_list):\n",
      "\u001b[1;32m/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb Cell 37\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m batch_size \u001b[39m=\u001b[39m img_tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m dec_input \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     [tokenizer\u001b[39m.\u001b[39mword_index[\u001b[39m'\u001b[39m\u001b[39m<start>\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m*\u001b[39m batch_size, \u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m features \u001b[39m=\u001b[39m extract_features(img_tensor)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m features \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(features, (features\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, features\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m]))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m features \u001b[39m=\u001b[39m encoder(features)\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1012\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_build(inputs)\n\u001b[1;32m   1010\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1014\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1015\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:424\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    407\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \n\u001b[1;32m    409\u001b[0m \u001b[39m  In this case `call` just reapplies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m      a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(\n\u001b[1;32m    425\u001b[0m       inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:560\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    557\u001b[0m   \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[0;32m--> 560\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mlayer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    562\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(node\u001b[39m.\u001b[39mflat_output_ids, nest\u001b[39m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1012\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_build(inputs)\n\u001b[1;32m   1010\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1014\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1015\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/merge.py:183\u001b[0m, in \u001b[0;36m_Merge.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n\u001b[1;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_merge_function(inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/merge.py:522\u001b[0m, in \u001b[0;36mConcatenate._merge_function\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_merge_function\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m--> 522\u001b[0m   \u001b[39mreturn\u001b[39;00m K\u001b[39m.\u001b[39;49mconcatenate(inputs, axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m    203\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    204\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:2989\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(tensors, axis)\u001b[0m\n\u001b[1;32m   2987\u001b[0m   \u001b[39mreturn\u001b[39;00m ragged_concat_ops\u001b[39m.\u001b[39mconcat(tensors, axis)\n\u001b[1;32m   2988\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2989\u001b[0m   \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39;49mconcat([to_dense(x) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m tensors], axis)\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m    203\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    204\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1677\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1673\u001b[0m     ops\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m   1674\u001b[0m         axis, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconcat_dim\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         dtype\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mget_shape()\u001b[39m.\u001b[39massert_has_rank(\u001b[39m0\u001b[39m)\n\u001b[1;32m   1676\u001b[0m     \u001b[39mreturn\u001b[39;00m identity(values[\u001b[39m0\u001b[39m], name\u001b[39m=\u001b[39mname)\n\u001b[0;32m-> 1677\u001b[0m \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49mconcat_v2(values\u001b[39m=\u001b[39;49mvalues, axis\u001b[39m=\u001b[39;49maxis, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:1189\u001b[0m, in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   1188\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   1190\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mConcatV2\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, values, axis)\n\u001b[1;32m   1191\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   1192\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "EPOCHS = 10\n",
    "start = time.time()\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataset_training, total=num_steps, desc=f'Epoch {epoch + 1:2d}')\n",
    "    for (step, (img_tensor, target)) in enumerate(pbar):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        pbar.set_postfix({'loss': total_loss.numpy() / (step + 1)})\n",
    "\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "    ckpt_manager.save()\n",
    "\n",
    "    score = evaluate(dataset_valid)\n",
    "    print(f'Validation accuracy: {score:.2f}')\n",
    "\n",
    "print('Time taken for {} epoch {} sec\\n'.format(EPOCHS - start_epoch, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_test(img_name):\n",
    "    img = tf.io.read_file(img_name)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, (160, 300))\n",
    "    img = img/255*2-1\n",
    "    return img, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_testing = tf.data.Dataset.from_tensor_slices(img_test)\\\n",
    "                                .map(map_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                                .batch(50)\\\n",
    "                                .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the result to csv file, path = './Lab12-2_112062524.txt'\n",
    "# the format of the csv file is:\n",
    "# img_name - \"./dataset/words_captcha/\" , answer\n",
    "with open('./Lab12-2_112062524.txt', 'w') as f:\n",
    "    f.write('img_name,answer\\n')\n",
    "    for img_tensor, img_name in dataset_testing:\n",
    "        pred_list = postprocess(predict(img_tensor).numpy())\n",
    "        for img, pred in zip(img_name.numpy(), pred_list):\n",
    "            f.write(f'{img.decode()}, {pred}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the \"./dataset/words_captcha/\" and \".png\" from the img_name\n",
    "df = pd.read_csv('./Lab12-2_112062524.txt')\n",
    "df['img_name'] = df['img_name'].apply(lambda x: x[24:-4])\n",
    "df.to_csv('./Lab12-2_112062524_2.txt', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the Lab12-2_112062524_2.txt and delete all \",\" in the file\n",
    "# then save the file to Lab12-2_112062524_3.txt\n",
    "with open('./Lab12-2_112062524_2.txt', 'r') as f:\n",
    "    contents = f.read()\n",
    "contents = contents.replace(',', '')\n",
    "\n",
    "with open('./Lab12-2_112062524_3.txt', 'w') as f:\n",
    "    f.write(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "## Data Preprocess\n",
    "- 在 preprocess 的部分, 基本上就是用lab的內容, 並按照 hint 做處理, 並沒有做額外的處理\n",
    "## Encoder Decoder\n",
    "- 在 decoder 跟 encoder 的部分是把例子的直接拿來用 , 沒有做額外的更改(一部分也是因為我其實對\n",
    "RNN 不太熟, 也不知道要怎麼更改)\n",
    "\n",
    "## Model\n",
    "- 主要用了兩個不同的 pretrain model 來實作 feature extractor, 分別是 DenseNet121 跟 resNet50,\n",
    "- DenseNet121 : 可以從 training 結果看到如果用 DenseNet 在第一個 epoch 結束時整體的 performance 就非常高了, 會使用 denseNet 是因為看到了下面這篇文章\n",
    "  - https://www.aimspress.com/fileOther/PDF/MBE/mbe-16-05-292.pdf\n",
    "- ResNet50 : 會使用 ResNet 則是因為在過去修ML時有印象 ResNet50 的 performance 也非常優秀, 所以這次就好奇 resnet 的表現會如何, 可以看到 ResNet 的表現其實比 DenseNet 還差, 但是這其實也跟文獻上的結果一樣, 但是仍然能達到此次 lab 的要求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
