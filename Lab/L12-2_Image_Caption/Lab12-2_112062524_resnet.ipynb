{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 18:53:08.101408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 18:53:11.381468: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-11-30 18:53:11.382906: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-11-30 18:53:11.398268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2023-11-30 18:53:11.398648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:82:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2023-11-30 18:53:11.399012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: \n",
      "pciBusID: 0000:85:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2023-11-30 18:53:11.399369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: \n",
      "pciBusID: 0000:86:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2023-11-30 18:53:11.399405: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-11-30 18:53:11.403547: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-11-30 18:53:11.403633: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-11-30 18:53:11.405983: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-11-30 18:53:11.406464: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-11-30 18:53:11.408982: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-11-30 18:53:11.410044: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-11-30 18:53:11.410396: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-11-30 18:53:11.412854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2023-11-30 18:53:11.421245: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-11-30 18:53:11.421699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:82:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2023-11-30 18:53:11.421737: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-11-30 18:53:11.421769: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-11-30 18:53:11.421795: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-11-30 18:53:11.421819: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-11-30 18:53:11.421844: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-11-30 18:53:11.421869: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-11-30 18:53:11.421894: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-11-30 18:53:11.421918: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-11-30 18:53:11.422718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 1\n",
      "2023-11-30 18:53:11.422770: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-11-30 18:53:12.096739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-11-30 18:53:12.096780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      1 \n",
      "2023-11-30 18:53:12.096788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N \n",
      "2023-11-30 18:53:12.097706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14957 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0, compute capability: 6.0)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, AveragePooling2D, BatchNormalization, Activation, Concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications import ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from './dataset/words_captcha/spec_train_val.txt'\n",
    "import glob\n",
    "\n",
    "img_list = []\n",
    "answer_list = []\n",
    "\n",
    "with open('./dataset/words_captcha/spec_train_val.txt', 'r') as f:\n",
    "  for line in f:\n",
    "    image, answer = line.strip().split()\n",
    "    answer_list.append('<start> ' + ' '.join(answer) + ' <end>')\n",
    "    img_list.append('./dataset/words_captcha/' + image+'.png')\n",
    "\n",
    "# so the words_captcha has some image that is not list in  spec_train_val.txt, \n",
    "# we need to add these image to the img_list\n",
    "tmp = set(glob.glob(f'./dataset/words_captcha/*.png')) - set(img_list)\n",
    "# sort the tmp \n",
    "tmp = sorted(tmp)\n",
    "img_list += tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<unk>\", \n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(answer_list)\n",
    "answer_seqs = tokenizer.texts_to_sequences(answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenized vectors\n",
    "answer_seqs = tokenizer.texts_to_sequences(answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions\n",
    "answer_vector = tf.keras.preprocessing.sequence.pad_sequences(answer_seqs, padding='post')\n",
    "max_length = calc_max_length(answer_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> t h u s <end>', '<start> w w w <end>', '<start> t i e d <end>', '<start> i d s <end>', '<start> j a m <end>', '<start> z o o <end>', '<start> a p p l e <end>', '<start> b i g <end>', '<start> l o t <end>', '<start> a b o v e <end>']\n",
      "[[ 2  9 18 17  6  3  0]\n",
      " [ 2 24 24 24  3  0  0]\n",
      " [ 2  9  8  4 13  3  0]\n",
      " [ 2  8 13  6  3  0  0]\n",
      " [ 2 26  5 16  3  0  0]\n",
      " [ 2 28  7  7  3  0  0]\n",
      " [ 2  5 15 15 11  4  3]\n",
      " [ 2 20  8 19  3  0  0]\n",
      " [ 2 11  7  9  3  0  0]\n",
      " [ 2  5 20  7 25  4  3]]\n"
     ]
    }
   ],
   "source": [
    "print(answer_list[:10])\n",
    "print(answer_vector[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the first 100,000 images as training data, the next 20,000 as validation data, and the rest (final 20,000) as testing data.\n",
    "img_train, img_valid, img_test = img_list[:100000], img_list[100000:120000], img_list[120000:]\n",
    "answer_train, answer_valid = answer_vector[:100000], answer_vector[100000:120000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 20000, 20000, 100000, 20000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_train), len(img_valid), len(img_test), len(answer_train), len(answer_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "BUFFER_SIZE = 5000\n",
    "LEARNING_RATE = 1e-4\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_steps = len(img_train) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(img_name, cap):\n",
    "    img = tf.io.read_file(img_name)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = img/255*2-1\n",
    "    img = tf.image.resize(img, (160, 300))\n",
    "    return img, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_training = tf.data.Dataset.from_tensor_slices((img_train, answer_train))\n",
    "dataset_training = dataset_training.map(lambda item1, item2: tf.numpy_function(\n",
    "                    map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "                    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset_training = dataset_training.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_valid = tf.data.Dataset.from_tensor_slices((img_valid, answer_valid))\n",
    "dataset_valid = dataset_valid.map(lambda item1, item2: tf.numpy_function(\n",
    "                    map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "                    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset_valid = dataset_valid.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features = ResNet50(include_top=False, weights='imagenet', input_shape=(160, 300, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder / Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n",
    "\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  \n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/ResNet50\"\n",
    "ckpt = tf.train.Checkpoint(model=extract_features,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "    hidden = decoder.reset_state(batch_size=BATCH_SIZE) \n",
    "\n",
    "    with tf.GradientTape()as tape:\n",
    "        features = extract_features(img_tensor)\n",
    "        features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "        features = encoder(features)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "    \n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables + extract_features.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_tensor):\n",
    "    batch_size = img_tensor.shape[0]\n",
    "    dec_input = tf.expand_dims(\n",
    "        [tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "    features = extract_features(img_tensor)\n",
    "    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "\n",
    "    result = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    for _ in range(max_length):\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions, axis=1).numpy()\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "        result = tf.concat([result, predicted_id.reshape((batch_size, 1))], axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "def postprocess(segs):\n",
    "    result_list = []\n",
    "    for seq in segs:\n",
    "        result = ''\n",
    "        for s in seq[1:]:\n",
    "            if s == tokenizer.word_index['<end>']:\n",
    "                break\n",
    "            result += tokenizer.index_word[s]\n",
    "        result_list.append(result)\n",
    "    return result_list\n",
    "\n",
    "def evaluate(dataset_valid):\n",
    "    sample_count = 0\n",
    "    correct_count = 0\n",
    "    for img_tensor, target in dataset_valid:\n",
    "        pred_list = postprocess(predict(img_tensor).numpy())\n",
    "        real_list = postprocess(target.numpy())\n",
    "\n",
    "        for pred, real in zip(pred_list, real_list):\n",
    "            sample_count += 1\n",
    "            if pred == real:\n",
    "                correct_count += 1\n",
    "    print(f\"sample_count: {sample_count}, correct_count: {correct_count}\")\n",
    "    return correct_count / sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1:   0%|          | 0/2000 [00:00<?, ?it/s]2023-11-30 18:57:10.748892: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-11-30 18:57:10.975356: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-11-30 18:57:11.009146: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "Epoch  1: 100%|██████████| 2000/2000 [10:26<00:00,  3.19it/s, loss=1.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_count: 20000, correct_count: 0\n",
      "Validation accuracy: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2: 100%|██████████| 2000/2000 [10:13<00:00,  3.26it/s, loss=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_count: 20000, correct_count: 8\n",
      "Validation accuracy: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 2000/2000 [10:13<00:00,  3.26it/s, loss=0.7]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_count: 20000, correct_count: 16481\n",
      "Validation accuracy: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4: 100%|██████████| 2000/2000 [10:13<00:00,  3.26it/s, loss=0.0797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_count: 20000, correct_count: 17578\n",
      "Validation accuracy: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5: 100%|██████████| 2000/2000 [10:13<00:00,  3.26it/s, loss=0.0457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_count: 20000, correct_count: 18472\n",
      "Validation accuracy: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6: 100%|██████████| 2000/2000 [10:13<00:00,  3.26it/s, loss=0.0343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_count: 20000, correct_count: 18800\n",
      "Validation accuracy: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7: 100%|██████████| 2000/2000 [10:13<00:00,  3.26it/s, loss=0.0257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_count: 20000, correct_count: 18848\n",
      "Validation accuracy: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8: 100%|██████████| 2000/2000 [10:14<00:00,  3.26it/s, loss=0.0215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_count: 20000, correct_count: 18779\n",
      "Validation accuracy: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9:   2%|▏         | 38/2000 [00:12<10:31,  3.10it/s, loss=0.0197]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524_resnet.ipynb Cell 29\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524_resnet.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(dataset_training, total\u001b[39m=\u001b[39mnum_steps, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m2d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524_resnet.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m (step, (img_tensor, target)) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(pbar):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524_resnet.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     batch_loss, t_loss \u001b[39m=\u001b[39m train_step(img_tensor, target)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524_resnet.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m t_loss\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B140.114.91.178/home/mygodimatomato/DeepLearning/Lab/L12-2_Image_Caption/Lab12-2_112062524_resnet.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     pbar\u001b[39m.\u001b[39mset_postfix({\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m: total_loss\u001b[39m.\u001b[39mnumpy() \u001b[39m/\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)})\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    827\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name) \u001b[39mas\u001b[39;00m tm:\n\u001b[0;32m--> 828\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    829\u001b[0m   compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experimental_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m   new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:855\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    852\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    853\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    854\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    857\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    858\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2940\u001b[0m   (graph_function,\n\u001b[1;32m   2941\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2943\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1914\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1915\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1916\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1917\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1919\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1920\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m     args,\n\u001b[1;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1923\u001b[0m     executing_eagerly)\n\u001b[1;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    554\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    556\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    557\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    558\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    559\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    560\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    561\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    562\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    563\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    564\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    568\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/for_tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "EPOCHS = 10\n",
    "start = time.time()\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataset_training, total=num_steps, desc=f'Epoch {epoch + 1:2d}')\n",
    "    for (step, (img_tensor, target)) in enumerate(pbar):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        pbar.set_postfix({'loss': total_loss.numpy() / (step + 1)})\n",
    "\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "    ckpt_manager.save()\n",
    "\n",
    "    score = evaluate(dataset_valid)\n",
    "    print(f'Validation accuracy: {score:.2f}')\n",
    "\n",
    "print('Time taken for {} epoch {} sec\\n'.format(EPOCHS - start_epoch, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_test(img_name):\n",
    "    img = tf.io.read_file(img_name)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, (160, 300))\n",
    "    img = img/255*2-1\n",
    "    return img, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_testing = tf.data.Dataset.from_tensor_slices(img_test)\\\n",
    "                                .map(map_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                                .batch(50)\\\n",
    "                                .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the result to csv file, path = './Lab12-2_112062524.txt'\n",
    "# the format of the csv file is:\n",
    "# img_name - \"./dataset/words_captcha/\" , answer\n",
    "with open('./Lab12-2_112062524_r_2.txt', 'w') as f:\n",
    "    f.write('img_name,answer\\n')\n",
    "    for img_tensor, img_name in dataset_testing:\n",
    "        pred_list = postprocess(predict(img_tensor).numpy())\n",
    "        for img, pred in zip(img_name.numpy(), pred_list):\n",
    "            f.write(f'{img.decode()}, {pred}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the \"./dataset/words_captcha/\" and \".png\" from the img_name\n",
    "df = pd.read_csv('./Lab12-2_112062524_r_2.txt')\n",
    "df['img_name'] = df['img_name'].apply(lambda x: x[24:-4])\n",
    "df.to_csv('./Lab12-2_112062524_3.txt', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the Lab12-2_112062524_2.txt and delete all \",\" in the file\n",
    "# then save the file to Lab12-2_112062524_3.txt\n",
    "with open('./Lab12-2_112062524_3.txt', 'r') as f:\n",
    "    contents = f.read()\n",
    "contents = contents.replace(',', '')\n",
    "\n",
    "with open('./Lab12-2_112062524_resnet.txt', 'w') as f:\n",
    "    f.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
